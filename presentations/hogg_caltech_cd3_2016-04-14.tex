% This file is part of The Cannon.
% Copyright 2016 David W. Hogg (NYU).

\documentclass[pdftex]{beamer}
\input{vc}
% 1.77778 is the ratio of 16 to 9
\setlength{\paperheight}{3.0in}
\setlength{\paperwidth}{1.77778\paperheight}
% 1.33333 is the ratio of 4 to 3
%\setlength{\paperheight}{3.2in}
%\setlength{\paperwidth}{1.33333\paperheight}
% textwidth
\setlength{\textwidth}{0.85\paperwidth}
% import the next thing *after* the papersize
\input{hogg_presentation} % hogg standard colors

\newcommand{\credits}{{\footnotesize (Ness, Hogg, \etal)}}
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe / H}]}

\title{A new model for stars}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\footnotesize Simons Center for Data Analysis, New York} \\
  \textsl{\footnotesize Center for Cosmology and Particle Physics, Dept.~Physics, NYU} \\
  \textsl{\footnotesize Center for Data Science, NYU} \\
  \textsl{\footnotesize Max-Planck-Insitut f\"ur Astronomie, Heidelberg}}
\date{Caltech CD3 / 2016 April 14}

\newcommand{\conclusions}{%
\begin{frame}
  \frametitle{conclusions}
  \begin{itemize}
  \item \emph{A data-driven model provides physical parameters to stars with better quality than any physics-driven pipeline.}
    \begin{itemize}
    \item \tc ---no physics is harmed
    \end{itemize}
  \item Prospects for exoplanet and Milky Way science.
  \item Everything open-source.
  \item \emph{Melissa~Ness}~(MPIA), \emph{Andy~Casey}~(Cambridge), \emph{Anna~Y.~Q.~Ho}~(Caltech), Hans-Walter~Rix~(MPIA)
  \end{itemize}
\end{frame}}

\begin{document}\sloppy\sloppypar\raggedright\raggedbottom

\begin{frame}
  \titlepage
\end{frame}

\conclusions

\begin{frame}
  \frametitle{Annie Jump Cannon}
  \begin{itemize}
  \item O B A F G K M
    \begin{itemize}
    \item temperature sequence!
    \item alphabetical order (A B F G K M O) is hydrogen-line-strength order
    \end{itemize}
  \item Cannon understood the temperature sequence of stars without the benefit of physical models
    \begin{itemize}
    \item data-driven non-linear dimensionality reduction
    \item ``manifold learning''
    \item (using a huge amount of prior knowledge)
    \end{itemize}
  \item namesake of \tc
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Stars, a potted history}
  \begin{itemize}
  \item in the 1800s, Bunsen, Kirchoff, others.
  \item classification A--O based on line strengths.
  \item Annie Jump Cannon reorders the classification purely morphologically.
  \item 1905-ish: First one-dimensional (spherically symmetric) models.
  \item 1980s: Models that can accurately describe hundreds of atomic lines.
  \item today: percent-level physical models for stellar spectra, millions-of-modes helioseismic model.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{chemodynamics}
  \begin{itemize}
  \item stars populate orbits in the Milky Way
    \begin{itemize}
    \item (nearly) conserved Hamiltonian ``actions''
    \end{itemize}
  \item stars are formed from particular gas clouds
    \begin{itemize}
    \item stars have conserved surface abundances
    \end{itemize}
  \item the combined action-chemical space will be far more
    informative than either taken independently
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{exoplanets}
  \begin{itemize}
  \item extra-solar planets are always measured relative to their host stars
  \item you only understand a planet \emph{as well as you can understand the star}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{the paradox of precision astrophysics}
  \begin{itemize}
  \item models are incredibly \emph{explanatory}
    \begin{itemize}
    \item physical cosmology
    \item stellar spectroscopy
    \item helioseismology
    \end{itemize}
  \item and yet...
  \item<2-> models are \emph{wrong} (ruled out) in detail
    \begin{itemize}
    \item $\chi^2 \gg \nu$
    \item ``The $\chi^2$ statistic is a measure of the size of your data!''
    \end{itemize}
  \item<2-> missing physics, approximation, computation, \emph{gastrophysics}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{physics-driven models}
  \begin{itemize}
  \item put in everything you know
    \begin{itemize}
    \item gravity, atomic and molecular transitions, radiation
    \end{itemize}
  \item make approximations to make things computable
    \begin{itemize}
    \item ``sub-grid'' models, mixing length, etc
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{``machine learning''}
  \begin{itemize}
  \item the most extreme of data-driven models
  \item ``the data \emph{is} the model''
    \begin{itemize}
    \item none of your knowledge is relevant
    \end{itemize}
  \item learn (fit) an exceedingly flexible model
    \begin{itemize}
    \item explain or cluster the data
    \item transformation from data to ``labels''
    \end{itemize}
  \item concept of non-parametrics
  \item concept of train, validate, and test
  \item many packages and implementations
    \begin{itemize}
    \item (and outrageous successes)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{when does ``machine learning'' help you?}
  \begin{itemize}
  \item train \& test situation
  \item training data are statistically identical to the test data
    \begin{itemize}
    \item same noise amplitude
    \item same distance or redshift distribution
    \item same luminosity distribution
    \item \emph{never true!}
    \end{itemize}
  \item training data have accurate and precise labels
  \item therefore, we \emph{can't use vanilla machine learning!}
    \begin{itemize}
    \item (astronomers rarely can)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{data-driven models (my personal usage)}
  \begin{itemize}
  \item make use of things you \emph{strongly believe}
    \begin{itemize}
    \item noise model \& instrument resolution
    \item causal structure (shared parameters)
    \end{itemize}
  \item capitalize on huge amounts of data
  \item exceedingly flexible model
  \item concept of train, validate, and test
  \item every situation will be \emph{bespoke}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{label transfer for stars}
  \begin{itemize}
  \item a few of your stars have good labels (from somewhere)
  \item can you use this to label the other stars?
  \item why would you want to do this?
    \begin{itemize}
    \item<2> you don't have good models at your wavelengths?
    \item<2> you want two surveys to be on the same ``system''?
    \item<2> you have some stars at high SNR, some at low SNR?
    \item<2> you spent human time on some stars but can't on all?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{stellar spectra}
  \begin{itemize}
  \item<1-> stars are very close to black-bodies
  \item<1-> to first order, a stellar spectrum depends on \emph{effective temperature} $\teff$ and \emph{surface gravity} $\logg$
  \item<2-> to second order, \emph{metallicity} $\feh$
  \item<3> to third order, tens of \emph{chemical abundances}, rotation
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{stellar spectra}
  \begin{itemize}
  \item all chemical information is in \emph{absorption lines} corresponding to atomic and molecular transitions
  \item some 30 elements are visible in the best stars
  \item spectroscopy at $$R\equiv\frac{\lambda}{\Delta\lambda}>20,000$$ is the primary tool
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{stellar astrophysics}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/four_examples3.pdf}
         \includegraphics<2>[height=\figureheight]{../documents/paper1/plots/iso2_2.png}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  \begin{itemize}
  \item Galactic archaeology
  \item \apogee\ DR10: 56,000 stars
  \item \apogee\ DR12: 156,000 stars (98,000 giants)
  \item $R=22,500$ spectra in $1.5<\lambda<1.7\,\mu\mathrm{m}$
  \item precise RVs and stellar parameters
  \item plan for a dozen abundances for every star
  \item (our own home-built and special continuum normalization; ask me!)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  \,\hfill\includegraphics[height=\figureheight]{../documents/paper1/plots/four_examples3.pdf}
\end{frame}

\begin{frame}
  \frametitle{train, validate, and test}
  \begin{itemize}
  \item split the data into three disjoint subsets
  \item in the \emph{training step} you set the parameters of your model using the training set
  \item the validation set is used to set hyperparameters or model complexity
  \item in the \emph{test step} you apply the model to the test set---new data---to make predictions or deliver results
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: training set}
  \begin{itemize}
  \item 543 stars (too few) from 19 clusters (too few)
  \item $\teff, \logg, \feh$ labels from \apogee
    \begin{itemize}
    \item calling parameters and abundances ``labels''
    \item slight adjustments to labels to get them onto possible isochrones
    \end{itemize}
  \item \emph{terrible} coverage of the main sequence
    \begin{itemize}
    \item only the Pleiades
    \item home-made Pleiades labels (by Ness)
    \item no $\feh$ spread at high $\logg$.
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: training set}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/training_aspcap2.pdf}
         \includegraphics<2>[height=\figureheight]{../documents/paper1/plots/training_mkn2.pdf}
\end{frame}

\newcommand{\flux}{f}
\newcommand{\fluxes}{\boldsymbol{\flux}}
\newcommand{\labels}{\boldsymbol{\ell}}
\newcommand{\pars}{\boldsymbol{\theta}}

\begin{frame}
  \frametitle{\tc: model}
  \begin{itemize}
  \item a \emph{generative model} of the \apogee\ spectra
    \begin{itemize}
    \item given label vector $\labels$, predict flux vector $\fluxes$
    \item probabilistic prediction $p(\fluxes\given\labels,\pars)$
    \end{itemize}
  \item use every spectral pixel's uncertainty variance $\sigma^2_{\lambda n}$ responsibly
  \item details:
    \begin{itemize}
    \item spectral expectation is quadratic in the labels
    \item every wavelength $\lambda$ treated independently
    \item an intrinsic Gaussian scatter $s^2_\lambda$ at every wavelength $\lambda$
    \item 80,000 free parameters in $\pars$!
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: model}
{\footnotesize
  \begin{eqnarray}
    \ln p(\fluxes_n\given\labels_n,\pars) &=& \sum_{\lambda=1}^L \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda)
    \nonumber \\
    \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda) &=& -\frac{1}{2}\,\frac{[f_{\lambda n} - \transpose{\pars_\lambda}\cdot\labels_n]^2}{\sigma^2_{\lambda n} + s^2_\lambda} + \ln (\sigma^2_{\lambda n} + s^2_\lambda)
    \nonumber \\
    \transpose{\labels} &\equiv& \left\{1, \teff, \logg, \feh,\right.
    \nonumber \\
                        & & \left.\teff^2, \teff\,\logg, \cdots, \feh^2\right\}
    \nonumber \\
    \transpose{\pars} &\equiv& \left\{\pars_\lambda, s^2_\lambda\right\}_{\lambda=1}^L
    \nonumber
  \end{eqnarray}
}
\end{frame}

\begin{frame}
  \frametitle{\tc: model}
  \begin{itemize}
  \item $\ln p(\fluxes_n\given\labels_n,\pars)$
  \item \emph{training step}: optimize w.r.t.\ parameters $\pars$ at fixed labels
    $\labels$ using training-set data
    \begin{itemize}
    \item linear least squares
    \item every wavelength $\lambda$ treated independently
    \end{itemize}
  \item \emph{test step}: optimize w.r.t.\ labels $\labels$ at fixed
    parameters $\pars$ using test-set (survey) data
    \begin{itemize}
    \item non-convex optimization
    \item every star treated independently
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: model training}
  \,\hfill\includegraphics<1>[width=\figurewidth]{./data_model_cyan.png}
\end{frame}

\begin{frame}
  \frametitle{\tc: model training}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/R1_continuum5.png}
\end{frame}

\begin{frame}
  \frametitle{\tc: model training cross-validation}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/takeout_histc.png}
\end{frame}

\newcommand{\results}{%
\begin{frame}
  \frametitle{\tc: results}
  \begin{itemize}
  \item \tc\ is far faster than physical modeling
    \begin{itemize}
    \item model trains in \emph{seconds} (thousands of fits)
    \item \tc\ labels $10^5$ stars per hour
    \item (pure Python on a laptop)
    \item physical modeling is \emph{three orders of magnitude} slower
    \end{itemize}
  \item labels appear sensible
    \begin{itemize}
    \item \tc\ labels lie near sensible isochrones
    \item scatter against \apogee\ labels consistent with \apogee\ precision
    \end{itemize}
  \end{itemize}
\end{frame}}

\results

\begin{frame}
  \frametitle{\tc: test time}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/4431_v19.png}
         \includegraphics<2>[height=\figureheight]{../documents/paper1/plots/4383_v19.png}
         \includegraphics<3>[height=\figureheight]{../documents/paper1/plots/4399_v19.png}
         \includegraphics<4>[height=\figureheight]{../documents/paper1/plots/4309_v19.png}
         \includegraphics<5>[height=\figureheight]{../documents/paper1/plots/4311_v19.png}
         \includegraphics<6>[height=\figureheight]{../documents/paper1/plots/4255_v19.png} 
\end{frame}

\begin{frame}
  \frametitle{\tc: comparison with \apogee\ labels}
  \,\hfill\includegraphics[height=\figureheight]{../documents/paper1/plots/cplot2.png} 
\end{frame}

\begin{frame}
  \frametitle{\tc: label veracity}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/iso2_2.png}
         \includegraphics<2>[height=\figureheight]{../documents/paper1/plots/iso2a_2.png}
\end{frame}

\begin{frame}
  \frametitle{\tc: works at low signal-to-noise}
  \,\hfill\includegraphics<1>[height=\figureheight]{../documents/paper1/plots/SNR100to200.png}
         \includegraphics<2>[height=\figureheight]{../documents/paper1/plots/SNR20to30.png}
\end{frame}

\results

\begin{frame}
  \frametitle{\tc: shortcuts and choices}
  \begin{itemize}
  \item no Bayes; no partial or noisy labels
  \item quadratic order
    \begin{itemize}
    \item replacing polynomial with a \emph{Gaussian process}
    \item continuous model complexity; non-parametric
    \end{itemize}
  \item spectral representation
  \item too-small training set
  \item only three labels
    \begin{itemize}
    \item age, [$\alpha$/Fe]
    \item splitting the giant branch
    \item how to go to many elements?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc: detailed abundances}
  \,\hfill\includegraphics<1>[                  width=\figurewidth]{regularized-model-validation.pdf}%
          \includegraphics<2>[trim=0 0 0 3.0in, width=\figurewidth]{regularized-model-validation.pdf}%
          \includegraphics<3>[trim=0 0 0 6.0in, width=\figurewidth]{regularized-model-validation.pdf}%
          \includegraphics<4>[trim=0 0 0 9.0in, width=\figurewidth]{regularized-model-validation.pdf}%
          \includegraphics<5>[                  width=0.5\figurewidth]{M15_comparison.pdf}%
          \includegraphics<6>[trim=0 0 0 4.0in, width=0.5\figurewidth]{M15_comparison.pdf}%
          \includegraphics<7>[trim=0 0 0 8.0in, width=0.5\figurewidth]{M15_comparison.pdf}%
          \includegraphics<8>[trim=0 0 0 12.in, width=0.5\figurewidth]{M15_comparison.pdf}%
          \includegraphics<9>[width=\figurewidth]{sparse-first-order-coefficients.pdf}%
          \includegraphics<10>[trim=0.5in 2in 0.5in 2in, width=\figurewidth]{sn.pdf}%
\end{frame}

\begin{frame}
  \frametitle{read more}
  \begin{itemize}
  \item original paper on \tc\ and \apogee: \emph{Ness} \etal, arXiv:1501.07604
  \item red-giant masses and ages: \emph{Ness} \etal, arXiv:1511.08204
  \item chemical tagging: Hogg \etal, arXiv:1601.05413
  \item labeling \project{\acronym{LAMOST}}: \emph{Ho} \etal, arXiv:1602.00303
  \item chemical abundances: \emph{Casey} \etal, arXiv:1603.03040
  \end{itemize}
\end{frame}

\conclusions

\end{document}
