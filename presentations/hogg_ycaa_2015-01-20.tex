\documentclass[pdftex]{beamer}
\input{hogg_presentation} % hogg standard colors
\input{vc}
\setlength{\paperheight}{3.5in}
% 1.77778 is the ratio of 16 to 9
\setlength{\paperwidth}{1.77778\paperheight}
% 1.33333 is the ratio of 4 to 3
% \setlength{\paperwidth}{1.33333\paperheight}
\setlength{\textwidth}{0.85\paperwidth}

\newcommand{\credits}{{\footnotesize (Ness, Hogg, Rix, \etal)}}
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe / H}]}

\title{Data-driven models of stars}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\small Center for Cosmology and Particle Physics,
                 New York University}}
\date{2015 January 20}

\newcommand{\conclusions}{%
\begin{frame}
  \frametitle{conclusions}
  \begin{itemize}
  \item A data-driven label transfer system provides \apogee\ stars with labels comparable in quality to the \apogee\ physics-driven pipeline.
    \begin{itemize}
    \item \tc\ (Ness \etal, \textit{ApJ} submitted)
    \item ``labels'' $\equiv (\teff,\logg,\feh)$
    \end{itemize}
  \item \tc\ uses \emph{no physical model of stars}.
  \item The method and the training set are both immature zeroth steps.
  \item There is hope for a consistent system of stellar parameters and chemical abundances across all future surveys.
    \begin{itemize}
    \item \gaia
    \item \sdss\ \project{SEGUE} \& \apogee
    \item \project{HERMES} / \project{GALAH}
    \end{itemize}
  \item \texttt{\giturl~~(\githash~\gitdate)}
  \end{itemize}
\end{frame}}

\begin{document}

\begin{frame}
  \titlepage
  in collaboration with \emph{Melissa Ness} (MPIA) and Hans-Walter Rix
\end{frame}

\conclusions

\begin{frame}
  \frametitle{Annie Jump Cannon}
  \begin{itemize}
  \item understood the temperature sequence of stars without the benefit of physical models
    \begin{itemize}
    \item data-driven non-linear dimensionality reduction
    \item ``manifold learning''
    \item (using a huge amount of prior knowledge)
    \end{itemize}
  \item namesake of \tc
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The paradox of precision astrophysics}
  \begin{itemize}
  \item models are incredibly \emph{explanatory}
    \begin{itemize}
    \item $\Lambda$CDM
    \item stellar spectroscopy
    \item helioseismology
    \end{itemize}
  \item and yet...
  \item<2-> models are \emph{wrong} (ruled out) in detail
    \begin{itemize}
    \item $\chi^2 \gg \nu$
    \item ``The $\chi^2$ statistic is a measure of the size of your data!''
    \end{itemize}
  \item<2-> approximations, computation, \emph{gastrophysics}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{physics-driven models}
  \begin{itemize}
  \item put in everything you know
    \begin{itemize}
    \item gravity, atomic and molecular transitions, radiation
    \end{itemize}
  \item make approximations to make things computable
    \begin{itemize}
    \item subgrid models, mixing length, etc
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{machine learning}
  \begin{itemize}
  \item the most extreme of data-driven models
  \item ``the data \emph{is} the model''
    \begin{itemize}
    \item none of your knowledge is relevant
    \end{itemize}
  \item learn (fit) an exceedingly flexible model
    \begin{itemize}
    \item explain or cluster the data
    \item transformation from data to ``labels''
    \end{itemize}
  \item concept of non-parametrics
  \item concept of train, validate, and test
  \item many packages and implementations to choose from
    \begin{itemize}
    \item (and outrageous successes)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{when does machine learning help you?}
  \begin{itemize}
  \item train \& test situation
  \item training data are statistically identical to the test data
    \begin{itemize}
    \item same noise amplitude
    \item same distance or redshift distribution
    \item same luminosity distribution
    \item \emph{never true!}
    \end{itemize}
  \item training data have accurate and precise labels
  \item therefore, we \emph{can't use vanilla machine learning!}
    \begin{itemize}
    \item (no astronomers ever can)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{data-driven models (my personal usage)}
  \begin{itemize}
  \item make use of things you \emph{strongly believe}
    \begin{itemize}
    \item noise model \& instrument resolution
    \item causal structure (shared parameters)
    \end{itemize}
  \item capitalize on huge amounts of data
  \item exceedingly flexible model
  \item concept of train, validate, and test
  \item every situation will be \emph{bespoke}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{why stellar parameters and chemical abundances?}
  \begin{itemize}
  \item top priority for many new projects
    \begin{itemize}
    \item \gaia
    \item \project{HERMES} \& \project{GALAH}
    \item \sdssiii\ \apogee
    \end{itemize}
  \item extended distribution functions
  \item chemical tagging
    \begin{itemize}
    \item the dynamics and formation of the Milky Way
    \end{itemize}
  \item terrifying inconsistencies in current approaches
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{why label transfer for stars}
  \begin{itemize}
  \item don't have good models at your wavelengths?
  \item want two surveys to be on the same ``system''?
  \item have some stars at high SNR, some at low SNR?
  \item spent human time on some stars but can't on all?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item home-built and special continuum normalization (ask me!)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: training set}
  \begin{itemize}
  \item 543 stars (too few) from 19 clusters (too few)
  \item $\teff, \logg, \feh$ labels from \apogee
    \begin{itemize}
    \item calling parameters and abundances ``labels''
    \item slight adjustments to labels to get them onto possible isochrones
    \end{itemize}
  \item \emph{terrible} coverage of the main sequence
    \begin{itemize}
    \item only the Pleiades
    \item home-made Pleiades labels (by Ness)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: training set}
  \includegraphics[height=0.9\textheight]{../documents/plots/training_aspcap2.pdf}
\end{frame}

\newcommand{\flux}{f}
\newcommand{\fluxes}{\boldsymbol{\flux}}
\newcommand{\labels}{\boldsymbol{\ell}}
\newcommand{\pars}{\boldsymbol{\theta}}

\begin{frame}
  \frametitle{\tc\ \credits: model}
  \begin{itemize}
  \item a \emph{generative model} of the \apogee\ spectra
    \begin{itemize}
    \item given label vector $\labels$, predict flux vector $\fluxes$
    \item probabilistic prediction $p(\fluxes\given\labels,\pars)$
    \end{itemize}
  \item use every spectral pixel's uncertainty variance $\sigma^2_{\lambda n}$ responsibly
  \item details:
    \begin{itemize}
    \item spectral expectation is quadratic in the labels
    \item every wavelength $\lambda$ treated independently
    \item an intrinsic Gaussian scatter $s^2_\lambda$ at every wavelength $\lambda$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: model}
  \begin{eqnarray}
    \ln p(\fluxes_n\given\labels_n,\pars) &=& \sum_{\lambda=1}^L \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda)
    \nonumber \\
    \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda,s^2_\lambda) &=& -\frac{1}{2}\,\frac{[\transpose{\pars_\lambda}\cdot\labels_n]^2}{\sigma^2_{\lambda n} + s^2_\lambda}
    \nonumber \\
    \transpose{\labels} &\equiv& \left\{1, \teff, \logg, \feh, \teff^2, \teff\,\logg, \cdots, \feh^2\right\}
    \nonumber \\
    \transpose{\pars} &\equiv& \left\{\pars_\lambda, s^2_\lambda\right\}_{\lambda=1}^L
    \nonumber
  \end{eqnarray}
  \begin{itemize}
  \item \emph{training step}: optimize parameters $\pars$ at fixed labels
    $\labels$ using training-set data
    \begin{itemize}
    \item linear least squares
    \item every wavelength $\lambda$ treated independently
    \end{itemize}
  \item \emph{test step}: optimize labels $\labels$ at fixed
    parameters $\pars$ using test-set (survey) data
    \begin{itemize}
    \item non-linear optimization
    \end{itemize}
  \end{itemize}
\end{frame}

% HOGG FIGURE SHOWING SOMETHING

\begin{frame}
  \frametitle{\tc\ \credits: results}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

% HOGG MANY FIGURES

\begin{frame}
  \frametitle{\tc\ \credits: shortcuts and choices}
  \begin{itemize}
  \item no Bayes; no partial or noisy labels
  \item quadratic order
  \item spectral representation
  \item too-small training set
  \item only three labels
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{other applications for data-driven models}
  \begin{itemize}
  \item building a consistent stellar parameter system for \gaia
  \item \kepler\ lightcurves
  \item CMB foregrounds
  \end{itemize}
\end{frame}

% HOGG PUT IN FIGURE HERE FROM DUN's WORK

\begin{frame}
  \frametitle{conservatism requires huge numbers of parameters}
  \begin{itemize}
  \item without enormous freedom, the model won't be data-driven
  \item abandon hope that the number of parameters will be less than the number of data points
  \item marginalization of nuisances
  \end{itemize}
\end{frame}

\conclusions

\end{document}
