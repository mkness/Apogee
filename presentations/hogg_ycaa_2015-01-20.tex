\documentclass[pdftex]{beamer}
\input{hogg_presentation} % hogg standard colors
\input{vc}
\setlength{\paperheight}{3.5in}
% 1.77778 is the ratio of 16 to 9
\setlength{\paperwidth}{1.77778\paperheight}
% 1.33333 is the ratio of 4 to 3
% \setlength{\paperwidth}{1.33333\paperheight}
\setlength{\textwidth}{0.85\paperwidth}
\usepackage{amssymb,amsmath,mathrsfs}

\title{Data-driven models of stars}
\author[David W. Hogg (NYU)]{David W. Hogg \\
  \textsl{\small Center for Cosmology and Particle Physics,
                 New York University}}
\date{2015 January 20}

\begin{document}

\begin{frame}
  \titlepage
  \texttt{\giturl~\githash~(\gitdate)} \\
  in collaboration with \emph{Melissa Ness} (MPIA) and Hans-Walter Rix
\end{frame}

\begin{frame}
  \frametitle{The paradox of precision astrophysics}
  \begin{itemize}
  \item models are incredibly explanatory
  \item models are wrong (ruled out) in detail
    \begin{itemize}
    \item $\Lambda$CDM
    \item stellar spectroscopy
    \item helioseismology
    \end{itemize}
  \item approximations, computation, gastrophysics
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{physics-driven models}
  \begin{itemize}
  \item put in everything you know
  \item make approximations to make things computable
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{machine learning}
  \begin{itemize}
  \item the most extreme of data-driven models
  \item ``the data \emph{is} the model''
    \begin{itemize}
    \item none of your knowledge is relevant
    \end{itemize}
  \item learn (fit) an exceedingly flexible model
    \begin{itemize}
    \item explain or cluster the data
    \item transformation from data to ``labels''
    \end{itemize}
  \item concept of non-parametrics
  \item concept of train, validate, and test
  \item many packages and implementations to choose from
    \begin{itemize}
    \item (and outrageous successes)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{data-driven models}
  \begin{itemize}
  \item \textit{(my personal usage)}
  \item make use of things you \emph{strongly believe}
    \begin{itemize}
    \item noise model \& instrument resolution
    \item causal structure (shared parameters)
    \end{itemize}
  \item capitalize on huge amounts of data
  \item exceedingly flexible model
  \item concept of train, validate, and test
  \item every situation will be \emph{bespoke}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{when does machine learning help you?}
  \begin{itemize}
  \item train \& test situation
  \item training data are statistically identical to the test data
    \begin{itemize}
    \item same noise amplitude
    \item same distance or redshift distribution
    \item same luminosity distributionn
    \item \emph{never true!}
    \end{itemize}
  \item (training data have accurate and precise labels)
  \item we \emph{can't use vanilla machine learning!}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Why stellar parameters and chemical abundances?}
  \begin{itemize}
  \item top priority for many new projects
    \begin{itemize}
    \item \gaia
    \item \project{HERMES} \& \project{GALAH}
    \item \sdssiii\ \apogee
    \end{itemize}
  \item extended distribution functions
  \item chemical tagging
    \begin{itemize}
    \item the dynamics and formation of the Milky Way
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\sdssiii\ \apogee}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

\newcommand{\credits}{{\footnotesize (Ness, Hogg, Rix, \etal)}}
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe / H}]}

\begin{frame}
  \frametitle{\tc\ \credits: training set}
  \begin{itemize}
  \item 543 stars (too few) from 19 clusters (too few)
  \item $\teff, \logg, \feh$ labels from \apogee
    \begin{itemize}
    \item calling parameters and abundances ``labels''
    \item slight adjustments to labels to get them onto possible isochrones
    \end{itemize}
  \item \emph{terrible} coverage of the main sequence
    \begin{itemize}
    \item only the Pleiades
    \item home-made Pleiades labels (by Ness)
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: training set}
  \includegraphics[height=0.9\textheight]{../documents/plots/training_aspcap2.pdf}
\end{frame}

\newcommand{\flux}{f}
\newcommand{\fluxes}{\boldsymbol{\flux}}
\newcommand{\labels}{\boldsymbol{\ell}}
\newcommand{\pars}{\boldsymbol{\theta}}

\begin{frame}
  \frametitle{\tc\ \credits: model}
  \begin{itemize}
  \item a \emph{generative model} of the \apogee\ spectra
    \begin{itemize}
    \item given label vector $\labels$, predict flux vector $\fluxes$
    \item probabilistic prediction $p(\fluxes\given\labels,\pars)$
    \end{itemize}
  \item use every spectral pixel's uncertainty variance $\sigma^2_{\lambda n}$ responsibly
  \item details:
    \begin{itemize}
    \item spectral expectation is quadratic in the labels
    \item every wavelength $\lambda$ treated independently
    \item an intrinsic Gaussian scatter $s^2_\lambda$ at every wavelength $\lambda$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: model}
  \begin{eqnarray}
    \ln p(\fluxes_n\given\labels_n,\pars) &=& \sum_\lambda \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda) \\
    \ln p(\flux_{\lambda n}\given\labels_n,\pars_\lambda) &=& -\frac{1}{2}\,\frac{[\transpose{\pars_\lambda}\cdot\labels_n]^2}{\sigma^2_{\lambda n} + s^2_\lambda} \\
    \labels_n &\equiv& \left[1, \teff, \logg, \feh, \teff^2, \teff\,\logg, \cdots, \feh^2\right] \\
  \end{eqnarray}
  \begin{itemize}
  \item \emph{training step}: optimize parameters $\pars$ at fixed labels
    $\labels$ using training-set data
  \item \emph{test step}: optimize labels $\labels$ at fixed
    parameters $\pars$ using test-set (survey) data
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: results}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{\tc\ \credits: shortcuts and choices}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{other applications for data-driven models}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{conservatism requires huge numbers of parameters}
  \begin{itemize}
  \item foo
  \item foo
    \begin{itemize}
    \item bar
    \item bar
    \end{itemize}
  \item foo
  \end{itemize}
\end{frame}

\end{document}
